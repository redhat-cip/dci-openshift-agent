---
- hosts: localhost
  become: false
  gather_facts: false
  vars_files:
    - ../group_vars/all
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  vars:
    ai_releases_dir: "{{ (ansible_env.HOME, 'releases') | join('/') }}"
  tasks:
    - block:
      - name: "Add Bastion host to provisioner group"
        add_host:
          hostname: "{{ item }}"
          groups:
            - provisioner
        loop: "{{ groups['bastions'] }}"
        when:
          - "'provisioner' not in groups"

      - name: Include installed software as components
        vars:
          ic_rpms: "{{ dci_rpms_to_components }}"
          ic_gits: "{{ dci_gits_to_components }}"
          ic_dev_gits: "{{ dev_gits_to_components }}"
        include_role:
          name: include-components

      - name: Set pullsecret from job_info
        set_fact:
          pull_secret: "{{ job_info.job.topic.data.pull_secret | default({}) | combine(openshift_secret | default({}), recursive=True) }}"

      - name: "Copy auths fact content to a file"
        copy:
          content: "{{ pull_secret | to_json }}"
          dest: "{{ dci_cluster_configs_dir }}/pull-secret.txt"
          mode: "0400"
          force: true

      - name: "Set dci_pullsecret_file fact"
        set_fact:
          dci_pullsecret_file: "{{ dci_cluster_configs_dir }}/pull-secret.txt"
          local_pull_secret_path: "{{ dci_cluster_configs_dir }}/pull-secret.txt"
        no_log: true

      - name: "Configure auths for docker compatible tools"
        block:
          - name: "Create temp directory for docker auths"
            tempfile:
              state: directory
            register: docker_conf

          - name: "Copy dci_pull_secret as config.json"
            copy:
              src: "{{ dci_pullsecret_file }}"
              dest: "{{ docker_conf.path }}/config.json"
              mode: "0644"
        when:
          - dci_pullsecret_file is defined

      - name: "Create releases dir in home directory if needed"
        file:
          path: "{{ ai_releases_dir }}"
          state: directory
          setype: "container_file_t"
          mode: "0775"
        when:
          - hostvars.localhost.provision_cache_store is undefined

      - name: Mirror release
        include_role:
          name: mirror-ocp-release
        vars:
          mor_version: "{{ version }}"
          mor_pull_url: "{{ version_pull_url }}"
          mor_cache_dir: "{{ hostvars.localhost.provision_cache_store | default(ai_releases_dir) }}"
          mor_auths_file: "{{ dci_pullsecret_file }}"
          mor_force: "{{ (dci_force_mirroring | default(false)) or (hostvars.localhost.build == 'candidate') | bool }}"
          mor_mirror_disk_images: "{{ dci_disconnected | default(False) | bool }}"
          mor_mirror_container_images: "{{ dci_disconnected | default(False) | bool }}"
          mor_write_custom_config: "{{ dci_disconnected | default(False) | bool }}"
          mor_build: "{{ build }}"
          mor_oc: "{{ oc_tool_path }}"

      - name: "Set override facts for AI"
        delegate_to: "{{ item }}"
        delegate_facts: true
        set_fact:
          openshift_full_version: "{{ hostvars.localhost.version }}"
          openshift_version: "{{ hostvars.localhost.version.split('.')[:2] | join('.') }}"
          supported_ocp_versions:
            - "{{ hostvars.localhost.version }}"
          os_images:
            - openshift_version: "{{ hostvars.localhost.version.split('.')[:2] | join('.') }}"
              cpu_architecture: x86_64
              url: "{{ hostvars.localhost.ocp_release_data.rhcos_images.metal_iso_location }}"
              rootfs_url: "{{ hostvars.localhost.ocp_release_data.rhcos_images.metal_pxe_rootfs_location }}"
              version: "{{ hostvars.localhost.ocp_release_data.rhcos_version }}"
          release_images:
            - openshift_version: "{{ hostvars.localhost.version.split('.')[:2] | join('.') }}"
              cpu_architecture: x86_64
              url: "{{ hostvars.localhost.ocp_release_data.container_image }}"
              version: "{{ hostvars.localhost.version }}"
              hash: "{{ hostvars.localhost.ocp_release_data.container_image.split('@')[1] }}"
        loop:
          - assisted_installer
          - registry_host
          - bastion
          - localhost

      - name: "Clone/update assisted-deploy repo"
        vars:
          git_repo: "{{ assisted_deploy_repo }}"
          git_ref: "{{ assisted_deploy_version }}"
        git:
          version: "{{ git_ref }}"
          repo: "{{ git_repo }}"
          dest: "{{ dci_cache_dir }}/assisted_deploy_repo"
          force: true
        # On RHEL8 git clone can sporadically fail with OpenSSL SSL_read:
        # SSL_ERROR_SYSCALL, errno 104.
        register: git_clone
        retries: 3
        delay: 10
        until: not git_clone.failed
        when:
          - "dci_cache_dir + '/assisted_deploy_repo/roles' in roles_path"
        tags:
          - clone_upstream_repos

      - name: "Prereq facts check"
        vars:
          ssh_public_check: "{{ not (generate_ssh_keys | default(True)) }}"
          mirror_certificate_check: "{{ ((use_local_mirror_registry | default(False)) == True) and ((setup_registry_service | default(True)) == False) }}"
        include_role:
          name: prereq_facts_check

      - name: "Populate image_hashes for relevant images"
        include_role:
          name: get_image_hash

      - name: "Parse openshift version"
        include_role:
          name: populate_mirror_registry
          tasks_from: var_check.yml

      - name: "Pull openshift binaries"
        include_role:
          name: populate_mirror_registry
          tasks_from: prerequisites.yml
        vars:
          downloads_path: "{{ hostvars.localhost.provision_cache_store | default(ai_releases_dir) }}"
          release_image_remote: "{{ ocp_release_data['container_image'].split('@')[0] }}"
          config_file_path: "{{ dci_cluster_configs_dir }}"
      rescue: &teardown_error
        - name: Run the teardown error handler
          block:
            - name: error
              dci_job:
                id: "{{ hostvars.localhost.job_id }}"
                status: "error"
              tags: [dci]
              delegate_to: localhost

            - block:
                - name: Run the teardown hooks
                  include_tasks: "{{ hookdir }}/hooks/teardown.yml"
                  loop: "{{ dci_config_dirs }}"
                  loop_control:
                    loop_var: hookdir

                - name: Run the teardown play
                  include_tasks: teardown.yml
              when: dci_teardown_on_failure|bool
              delegate_to: localhost
              ignore_unreachable: true
              ignore_errors: true
          always:
            - name: Run the error process
              include_tasks: failure.yml
          delegate_to: localhost
          ignore_unreachable: true

- name: Process KVM nodes
  hosts: bastion
  vars_files:
    - ../group_vars/all
  tasks:
    - block:
      - name: Process kvm nodes
        include_role:
          name: process_kvm_nodes
        when: (setup_vms | default(true))
      rescue: *teardown_error

- name: Provision VMS
  hosts: vm_hosts
  gather_facts: "{{ (setup_vms | default((kvm_nodes | default([])) | length | int >= 1)) | bool }}"
  vars:
    SETUP_VMS: "{{ setup_vms | default((kvm_nodes | default([])) | length | int >= 1) }}"
  vars_files:
    - ../group_vars/all
  tasks:
    - block:
      - name: Destroy vms
        include_role:
          name: destroy_vms
        when: SETUP_VMS | bool
        tags:
          - destroy_vms

      - name: Setup vm host network
        include_role:
          name: setup_vm_host_network
        when: (SETUP_VM_BRIDGE | default(SETUP_VMS)) | bool

      - name: Configure firewall for vm_bridge_zone
        block:
          - name: Move bridge to designated firewall zone
            community.general.nmcli:
              conn_name: "{{ vm_bridge_name }}"
              state: present
              zone: "{{ vm_bridge_zone }}"

          - name: Create Iptables NAT chain
            iptables:
              table: nat
              chain: POSTROUTING
              source: '{{ machine_network_cidr }}'
              destination: '! {{ machine_network_cidr }}'
              jump: MASQUERADE
              protocol: all
              comment: Ansible NAT Masquerade

          - name: Manage IPv4 forwarding
            sysctl:
              name: net.ipv4.ip_forward
              value: '1'
              state: present
              reload: True
        when:
          - (SETUP_VM_BRIDGE | default(SETUP_VMS)) | bool
          - vm_bridge_zone is defined
        become: true

      - name: Create vms
        include_role:
          name: create_vms
        when: SETUP_VMS | bool
        tags:
          - setup_vms
      rescue: *teardown_error

- name: Setup DNS Records
  hosts: dns_host
  gather_facts: "{{ (setup_dns_service | default(True)) | bool }}"
  vars:
    SETUP_DNS_SERVICE: "{{ setup_dns_service | default(True) }}"
    domain: "{{ cluster_name + '.' + base_dns_domain }}"
  vars_files:
    - ../group_vars/all
  tasks:
    - block:
        - name: Insert dns records
          include_role:
            name: insert_dns_records
          when: SETUP_DNS_SERVICE == True

        - name: Use /etc/hosts
          copy:
            content: |
              # /etc/NetworkManager/dnsmasq.d/02-add-hosts.conf
              # By default, the plugin does not read from /etc/hosts.
              # This forces the plugin to slurp in the file.
              #
              # If you didn't want to write to the /etc/hosts file.  This could
              # be pointed to another file.
              #
              addn-hosts=/etc/hosts
            dest: /etc/NetworkManager/dnsmasq.d/02-add-hosts.conf
            mode: '0644'
          become: true

        - name: Remove expand-hosts entry
          lineinfile:
            path: /etc/NetworkManager/dnsmasq.d/dnsmasq.{{ cluster_name }}.conf
            state: absent
            regexp: ^expand-hosts
          become: true

        - name: "Restart NetworkManager"
          service:
            name: NetworkManager
            state: restarted
          become: true

        - name: Validate dns records
          include_role:
            name: validate_dns_records
      rescue: *teardown_error

- name: Setup NTP
  hosts: ntp_host
  gather_facts: "{{ (setup_ntp_service | default(True)) | bool }}"
  vars:
    SETUP_NTP_SERVICE: "{{setup_ntp_service | default(True)}}"
  vars_files:
    - ../group_vars/all
  tasks:
    - block:
      - name: Setup NTP
        include_role:
          name: setup_ntp
        when: SETUP_NTP_SERVICE == True
      rescue: *teardown_error

- name: Install and http_store service
  hosts: http_store
  gather_facts: "{{ (setup_http_store_service | default(True)) | bool }}"
  vars:
    SETUP_HTTP_STORE_SERVICE: "{{ setup_http_store_service | default(True) }}"
  vars_files:
    - ../group_vars/all
  tasks:
    - block:
      - name: Setup http store
        include_role:
          name: setup_http_store
        when: SETUP_HTTP_STORE_SERVICE == True

      - name: Validate http store
        include_role:
          name: validate_http_store
      rescue: *teardown_error

- name: Play to populate image_hashes for relevant images
  hosts: localhost
  gather_facts: "{{ (setup_registry_service | default(True)) | bool }}"
  vars:
    destination_hosts:
      - registry_host
  vars_files:
    - ../group_vars/all
  tasks:
    - block:
      - name: pre-compute need to get hashes
        set_fact:
          run_get_hash: "{{ image_hashes | default({}) | length == 0  }}"

      - name: Get Image Hash
        include_role:
          name: get_image_hash
        when: run_get_hash
      rescue: *teardown_error

- name: Play to install and setup mirror registry
  hosts: registry_host
  collections:
    - containers.podman
    - community.crypto
    - community.general
    - ansible.posix
  gather_facts: "{{ (setup_registry_service | default(True)) | bool }}"
  vars:
    downloads_path: "{{ hostvars.localhost.provision_cache_store | default(ai_releases_dir) }}"
    config_file_path: "{{ dci_cluster_configs_dir }}"
    SETUP_REGISTRY_SERVICE: "{{ setup_registry_service | default(True)}}"
  vars_files:
    - ../group_vars/all
  tasks:
    - block:
      - name: Setup selfsigned cert
        include_role:
          name: setup_selfsigned_cert
        when: SETUP_REGISTRY_SERVICE == True

      - name: Set pull secret facts on the registry host
        set_fact:
          local_pull_secret_path: "{{ hostvars.localhost.local_pull_secret_path }}"
          pull_secret: "{{ hostvars.localhost.pull_secret }}"

      - name: Setup mirror registry
        include_role:
          name: setup_mirror_registry
        when: SETUP_REGISTRY_SERVICE == True

      - name: Populate mirror registry
        include_role:
          name: populate_mirror_registry
        when: SETUP_REGISTRY_SERVICE == True
      rescue: *teardown_error

- name: Setup TFTP
  hosts: tftp_host
  vars:
    SETUP_TFTP_SERVICE: "{{ setup_pxe_service | default(False) }}"
  vars_files:
    - ../group_vars/all
  tasks:
    - block:
        - name: Setup TFTP
          include_role:
            name: setup_tftp
          when: SETUP_TFTP_SERVICE == True
      rescue: *teardown_error

- import_playbook: crucible/deploy_assisted_installer_onprem.yml
  when: not ((use_agent_based_installer | default(false)) | bool)

- name: Deploy sushy tools
  hosts: vm_hosts
  gather_facts: "{{ (setup_sushy_tools | default(setup_vms | default(True))) | bool }}"
  vars:
    SETUP_SUSHY_TOOLS: "{{ setup_sushy_tools | default(setup_vms | default(True)) }}"
  vars_files:
    - ../group_vars/all
  tasks:
    - block:
      - name: Setup sushy tools
        include_role:
          name: setup_sushy_tools
        when: SETUP_SUSHY_TOOLS == True
      rescue: *teardown_error

- name: Clean-up nodes - Mainly Baremetal use cases
  hosts: bastion
  vars_files:
    - ../group_vars/all
  tasks:
    - block:
      - name: Erase bootloader to prevent old OS to boot
        delegate_to: "{{ item }}"
        become: true
        shell: |
          if grep 'Red Hat Enterprise Linux CoreOS' /etc/os-release; then
            for disk in /dev/sd?; do
              dd if=/dev/zero of=$disk bs=512 count=1
            done
          fi
        when:
          - dci_erase_bootloader_on_disk|default(False)|bool
          - dci_main is not defined or dci_main == 'install'
        with_items: "{{ groups['masters'] + groups['workers'] | default([]) }}"
        ignore_unreachable: true
        ignore_errors: true

      - name: Empty Console log files if present
        command: dd if=/dev/null of="{{ item }}"
        with_fileglob:
          - "/var/consoles/{{ cluster }}/{{ cluster }}*"
        when:
          - cluster is defined
        become: true
        ignore_errors: true
...

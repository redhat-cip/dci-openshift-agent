---
- name: Copy facts from localhost to bastion
  hosts: bastion
  tasks:
    - name: Set facts
      set_fact:
        pull_secret: "{{ hostvars['localhost']['pull_secret'] }}"

    - name: Set registry_repository fact for install playbooks in disconnected mode
      set_fact:
        registry_repository: >
          {{ hostvars[groups['registry_host'][0]].local_repo |
          default('ocp-'+ hostvars['localhost']['version'].split('.')[:2] |
          join('.') +'/'+ hostvars['localhost']['version'], true) }}
      when: dci_disconnected | default(False) | bool

- import_playbook: crucible/deploy_cluster_agent_based_installer.yml
  when: (use_agent_based_installer | default(true)) | bool

- import_playbook: crucible/deploy_cluster_assisted_installer.yml
  when: not ((use_agent_based_installer | default(true)) | bool)

- name: Reset vars for DCI
  hosts: bastion
  vars_files:
    - ../group_vars/all
  tasks:
    - name: Reset cluster var for DCI
      set_fact:
        cluster: "{{ cluster_name }}"

- name: Get kubeconfig
  hosts: localhost
  vars:
    secure: false
    ASSISTED_INSTALLER_BASE_URL:
      "{{ secure | ternary('https', 'http') }}://\
      {{ hostvars['assisted_installer']['host'] }}:\
      {{ hostvars['assisted_installer']['port'] }}\
      /api/assisted-install/v2"
    CLUSTER_ID: "{{ cluster_id | default(hostvars['bastion']['cluster_id']) }}"
    URL_ASSISTED_INSTALLER_CLUSTER: "{{ ASSISTED_INSTALLER_BASE_URL }}/clusters/{{ CLUSTER_ID }}"
    kube_filename: "kubeconfig"
    dest_dir: "{{ kubeconfig_dest_dir | default(ansible_env.HOME) }}"
    kubeconfig_path: "{{ dest_dir }}/{{ kube_filename }}"
    kubeadmin_vault_name: "kubeadmin-password"
  vars_files:
    - ../group_vars/all
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - block:
        - name: Download kubeconfig (onprem)
          get_url:
            url: "{{ URL_ASSISTED_INSTALLER_CLUSTER }}/downloads/credentials?file_name=kubeconfig"
            dest: "{{ kubeconfig_path }}"
            mode: 0664
          when: not (use_agent_based_installer | default(true)) | bool

        - name: Copy generated kubeconfig (ABI)
          copy:
            src: "{{ repo_root_path }}/generated/{{ cluster_name }}/auth/kubeconfig"
            dest: "{{ kubeconfig_path }}"
            mode: '0664'
          when: (use_agent_based_installer | default(true)) | bool

        - name: Perform simple connectivity check with oc
          shell: |
            {{ oc_tool_path }} explain pods
          register: result
          retries: 3
          delay: 10
          until: result.rc == 0

        - name: Check status of cluster operators
          block:
            - name: Wait up to 20 mins for cluster to become functional
              shell: |
                {{ oc_tool_path }} wait clusteroperators --all --for=condition=Available --timeout=20m
              register: result
              retries: 3
              delay: 10
              until: result.rc == 0
          rescue:
            - name: Get better info for failure message
              shell: oc get clusteroperators
              register: co_result

            - name: Fail Cluster
              fail:
                msg: |
                  Cluster has not come up correctly:
                    {{ co_result.stdout }}

        - name: Store admin credentials (onprem)
          when: not (use_agent_based_installer | default(true)) | bool
          block:
            - name: Get credentials
              uri:
                url: "{{ URL_ASSISTED_INSTALLER_CLUSTER }}/credentials"
                return_content: true
              register: credentials

            - name: Save credentials to file
              copy:
                content: "{{ credentials.json | to_yaml }}"
                dest: "{{ dest_dir }}/{{ kubeadmin_vault_name }}"
                mode: 0600

            - name: Save credentials to vault
              shell:
                cmd: "ansible-vault encrypt --vault-password-file {{ kubeadmin_vault_password_file_path }} {{ dest_dir }}/{{ kubeadmin_vault_name }}"
              when: (kubeadmin_vault_password_file_path is defined) and (kubeadmin_vault_password_file_path is file)

        - name: Store admin credentials (ABI)
          when: (use_agent_based_installer | default(true)) | bool
          block:
            - name: Read credentials
              slurp:
                src: "{{ repo_root_path }}/generated/{{ cluster_name }}/auth/kubeadmin-password"
              register: kubeadmin_password

            - name: Set credentials map
              set_fact:
                kubeadmin_credentials:
                  username: kubeadmin
                  password: "{{ kubeadmin_password.content | b64decode }}"
                  console_url: "https://console-openshift-console.apps.{{ cluster_name }}.{{ base_dns_domain }}"

            - name: Save credentials to file
              copy:
                content: "{{ kubeadmin_credentials | to_yaml }}"
                dest: "{{ dest_dir }}/{{ kubeadmin_vault_name }}"
                mode: 0600

            - name: Save credentials to vault
              shell:
                cmd: "ansible-vault encrypt --vault-password-file {{ kubeadmin_vault_password_file_path }} {{ dest_dir }}/{{ kubeadmin_vault_name }}"
              when: (kubeadmin_vault_password_file_path is defined) and (kubeadmin_vault_password_file_path is file)

        - name: Set kubeconfig_path for provisioner
          set_fact:
            kubeconfig_path: "{{ kubeconfig_path }}"
          delegate_to: "{{ groups['provisioner'] | first }}"
          delegate_facts: true

        - name: Check if all cluster-operators are running correctly
          community.kubernetes.k8s_info:
            kind: ClusterOperator
          register: clusteroperator_info
          vars:
            status_query: "resources[*].status.conditions[?type=='Available'].status"
            cluster_operators_available: "{{ clusteroperator_info | json_query(status_query) | flatten | unique }}"
          retries: 6
          delay: 10
          until: cluster_operators_available == ['True']

        - name: Check if ClusterVersion is Complete
          community.kubernetes.k8s_info:
            api: config.openshift.io/v1
            kind: ClusterVersion
            name: version
          register: cluster_version
          vars:
            status_query: "resources[*].status.history[?state=='Completed'].state"
            cluster_version_available: "{{ cluster_version | json_query(status_query) | flatten | unique }}"
          retries: 60
          delay: 10
          until: cluster_version_available == ['Completed']

      rescue:
        - name: Run the teardown failure handler
          block:
            - name: failure
              dci_job:
                id: "{{ hostvars.localhost.job_id }}"
                status: "failure"
              delegate_to: localhost
              tags: [dci]

            - name: Run the failure process for partners
              include_tasks: "{{ hookdir }}/hooks/failure.yml"
              loop: "{{ dci_config_dirs }}"
              loop_control:
                loop_var: hookdir
              ignore_errors: true

            - block:
                - name: Run the teardown hooks
                  include_tasks: "{{ hookdir }}/hooks/teardown.yml"
                  loop: "{{ dci_config_dirs }}"
                  loop_control:
                    loop_var: hookdir

                - name: Run the teardown play
                  include_tasks: teardown.yml
              when: dci_teardown_on_failure|bool
              delegate_to: localhost
              ignore_unreachable: true
              ignore_errors: true
          always:
            - name: Run the failure process
              include_tasks: failure.yml
          delegate_to: localhost
          ignore_unreachable: true

- name: Assisted installer Cleanup
  hosts: localhost
  vars_files:
    - ../group_vars/all
  tasks:
    - name: Run Assisted installer cleanup
      vars:
        repo_root_path: "{{ dci_cluster_configs_dir }}"
      include_role:
        name: redhatci.ocp.populate_mirror_registry
        tasks_from: cleanup.yml
...
